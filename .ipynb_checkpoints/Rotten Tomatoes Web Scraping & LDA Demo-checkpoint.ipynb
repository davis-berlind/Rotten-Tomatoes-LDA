{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Required Packages\n",
    "\n",
    "`BeautifulSoup` is one of pthon's main webscraping libraries. `numpy` and `pandas` are standard data manipulation packages (they will give you functionality that you are used to from `R`, e.g. data frames). `requests` allows you to interact with web pages. `re` handles regular expressions. `sklearn` is python's main machine learning library (it is amazing and the gold standard). `nltk` is for natural language processing, in our case for lemmatizing the words we will be scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Davis\n",
      "[nltk_data]     Berlind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Davis\n",
      "[nltk_data]     Berlind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Davis Berlind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Beautiful Soup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests, re, time\n",
    "from time import sleep\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scraping Plots from Rotten Tomatoes Top 100 Movies for Each Genre\n",
    "\n",
    "The webpage for each movie has a storyline section that we can scrape after figuring out the CSS tag. The Rotten Tomatoes movie webpages look like https://www.rottentomatoes.com/m/[code]/ where [code] is specific to each movie and we need to figure it out. For simplicity, we'll just create a list of the genres we want to scrape from and their associated urls; however the commented out code below can be used to scrape the top movies from all genres. To get our scraped data, we have to go into Chrome's developer tools to figure out what part of the website we are interested in. In this case, we want the anchor tags ('a') of class 'unstyled articleLink' and we will use the `.find_all(tag, class_)` method of our `BeautifulSoup` object to extract the relevant html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.a Get Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Links to different genre lists\n",
    "#url = \"https://www.rottentomatoes.com/top/bestofrt/\"\n",
    "#resp = requests.get(url)\n",
    "#soup = BeautifulSoup(resp.text)\n",
    "#soup = BeautifulSoup(str(soup.find_all(\"ul\", class_=\"dropdown-menu\")[0]))\n",
    "\n",
    "#genre_links = soup.find_all('a')\n",
    "#pattern = \"href=\\\"(.*)\\\">\"\n",
    "#genres = [re.search(pattern, str(el)).group(1) for el in genre_links if re.search(pattern, str(el)) is not None]\n",
    "\n",
    "# defining the list of genres we are interested in scraping\n",
    "genres = [\"/top/bestofrt/top_100_action__adventure_movies/\",            # action/adventure\n",
    "          \"/top/bestofrt/top_100_comedy_movies/\",                       # comedy\n",
    "          \"/top/bestofrt/top_100_drama_movies/\",                        # drama\n",
    "          \"/top/bestofrt/top_100_science_fiction__fantasy_movies/\",     # sci-fi/fantasy\n",
    "          \"/top/bestofrt/top_100_sports__fitness_movies/\",              # sports\n",
    "          \"/top/bestofrt/top_100_horror_movies/\"]                       # horror\n",
    "\n",
    "# initialize list of movie url extensions\n",
    "movie_pages = []\n",
    "\n",
    "# getting links to movie pages\n",
    "for genre in genres:\n",
    "    sleep(np.random.randint(low=1,high=3,size=1))   # not necessary, but just puts random time between requests so we don't get\n",
    "    url = \"https://www.rottentomatoes.com\" + genre  # flagged as a bot\n",
    "    resp = requests.get(url)                        # 1. connecting to the site\n",
    "    soup = BeautifulSoup(resp.text)                 # 2. creating a beatiful soup object from the html text\n",
    "    pattern = \"href=\\\"(/m.*)\\\">\"                    # 3. defining the regex pattern that allows us to extract links\n",
    "    \n",
    "    # 4. extract and clean relevant html into a list using python's list comprehension\n",
    "    links = soup.find_all(\"a\", class_ = \"unstyled articleLink\") \n",
    "    movie_links = [re.search(pattern, str(link)).group(1) for link in links if re.search(pattern, str(link)) is not None]\n",
    "    movie_pages.extend(movie_links)                 # 5. adding the cleaned links to the list we initialized\n",
    "    \n",
    "# remove duplicates\n",
    "movie_pages = list(set(movie_pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to go from this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html lang=\"en\" xmlns:fb=\"http://www.facebook.com/2008/fbml\" xmlns:og=\"http://opengraphprotocol.org/schema/\" >\\n\\t<head prefix=\"og: http://ogp.me/ns# flixstertomatoes: http://ogp.me/ns/apps/flixstertomatoes#\">\\n    <script src=\"//cdn.optimizely.com/js/594670329.js\"></script>\\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\\n    <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\">\\n\\n    <meta name=\"google-site-verification\" content=\"VPPXtECgUUeuATBacnqnCm4ydGO99reF-xgNklSbNbc\" />\\n\\n    <meta name=\"msvalidate.01\" content=\"034F16304017CA7DCF45D43850915323\" />\\n\\n    <link href=\"https://staticv2-4.rottentomatoes.com/static/images/iphone/apple-touch-icon.png\" rel=\"apple-touch-icon\" />\\n    <link href=\"https://staticv2-4.rottentomatoes.com/static/images/icons/favicon.ico\" rel=\"shortcut icon\" type=\"image/x-icon\" />\\n    <link href=\"https://staticv2-4.rottentomatoes.com/static/styles/css/rt_main.css\" rel=\"stylesheet\" />\\n\\n    <script type=\"applicati'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/m/discreet_charm_of_the_bourgeoisie',\n",
       " '/m/last_picture_show',\n",
       " '/m/jodorowskys_dune',\n",
       " '/m/1020662-suspiria',\n",
       " '/m/back_to_the_future']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_pages[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b Scraping Plots\n",
    "\n",
    "We are just going to repeat the approach we took above, but now we know from looking at the movie webpages that the synopses we are interested in are flagged with an `id = \"movieSynopsis\"` tag which we can now throw into `.find_all()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Like any other great sports story, 'Murderball' features fierce rivalry, stopwatch suspense, dazzling athletic prowess, larger-than-life personalities and triumph over daunting odds. But murderball, the original name for the full-contact sport now known as quad rugby, is played by quadriplegics in armored wheelchairs. 'Murderball' is a story like no other, told by men who see the world from a different angle. Quad rugby players have suffered injuries that have left them with limited function in all four limbs. Whether by car wreck, gunshot, fist fight, rogue bacteria or any of an endless list of possible misadventures, quad rugby's young men have found their lives dramatically altered. Watching them in action -- both on court and off -- smashes every stereotype one has ever had about the handicapped. It also redefines what it is to be a man, what it is to live a full life, and what it is to be a winner.\",\n",
       " 'Based on the best-selling novel by Robert Traver (the pseudonym for Michigan Supreme Court justice John D. Voelker), Anatomy of a Murder stars James Stewart as seat-of-the-pants Michigan lawyer Paul Biegler. Through the intervention of his alcoholic mentor, Parnell McCarthy (Arthur O\\'Connell), Biegler accepts the case of one Lt. Manion (Ben Gazzara), an unlovable lout who has murdered a local bar owner. Manion admits that he committed the crime, citing as his motive the victim\\'s rape of the alluring Mrs. Manion (Lee Remick). Faced with the formidable opposition of big-city prosecutor Claude Dancer (George C. Scott), Biegler hopes to win freedom for his client by using as his defense the argument of \"irresistible impulse.\" Also featured in the cast is Eve Arden as Biegler\\'s sardonic secretary, Katherine Grant as the woman who inherits the dead man\\'s business, and Joseph N. Welch -- who in real life was the defense attorney in the Army-McCarthy hearings -- as the ever-patient judge. The progressive-jazz musical score is provided by Duke Ellington, who also appears in a brief scene. Producer/director Otto Preminger once more pushed the envelope in Anatomy of a Murder by utilizing technical terminology referring to sexual penetration, which up until 1959 was a cinematic no-no. Contrary to popular belief, Preminger was not merely being faithful to the novel; most of the banter about \"panties\" and \"semen,\" not to mention the 11-hour courtroom revelation, was invented for the film. Anatomy of a Murder was filmed on location in Michigan\\'s Upper Peninsula.',\n",
       " \"In Skyfall, Bond's loyalty to M is tested as her past comes back to haunt her. As MI6 comes under attack, 007 must track down and destroy the threat, no matter how personal the cost. -- (C) Official Site\",\n",
       " \"A growing nation of genetically evolved apes led by Caesar is threatened by a band of human survivors of the devastating virus unleashed a decade earlier. They reach a fragile peace, but it proves short-lived, as both sides are brought to the brink of a war that will determine who will emerge as Earth's dominant species. (c) Fox\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plots = []\n",
    "\n",
    "for movie_page in movie_pages:\n",
    "    url = \"https://www.rottentomatoes.com\" + movie_page\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    plots.append(re.search(\"\\\\n(.*)\\\\n\", str(soup.find_all(id=\"movieSynopsis\")[0])).group(1).strip())   \n",
    "plots[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create functions for cleaning and 'lemmatizing' our plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_plots(plots):\n",
    "    \"\"\" cleans scraped list of documents to be ready for LDA \"\"\"\n",
    "    # remove all punctuation and send to lower case\n",
    "    no_punct = [plot.translate(str.maketrans('', '', string.punctuation)) for plot in plots]\n",
    "    pos_list = [\"JJ\", \"JJR\", \"JJS\", \"NN\", \"NNS\", \"NNPS\", \"RB\", \"RBR\", \"RBS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"]\n",
    "    clean = []\n",
    "    for plot in no_punct:\n",
    "        clean.append(' '.join([w for w in plot.split() if nltk.pos_tag([w])[0][1] in pos_list]))\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    cleaner = []\n",
    "    # lemmatize documents\n",
    "    for plot in clean:\n",
    "        cleaner.append(' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(plot)]))\n",
    "    return cleaner\n",
    "\n",
    "plots_lemmatized = lemmatize_plots(plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize a `CountVectorizer` object, which creates a matrix of word counts that we will pass to the `LatentDirichletAllocation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=0.01,                   # eliminate words occuring in less than this proportion of docs\n",
    "                             max_df=0.7,                    # eliminate words occuring in more than this proportion of docs \n",
    "                             stop_words='english',          # remove stop words\n",
    "                             token_pattern='[a-zA-Z]{3,}',  # num chars > 3, no numbers\n",
    "                             #max_features=50000            # max number of uniq words\n",
    "                            )\n",
    "\n",
    "vectorized_plots = vectorizer.fit_transform(plots_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we specify our LDA model, fit it, and report our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>voice</td>\n",
       "      <td>murder</td>\n",
       "      <td>dead</td>\n",
       "      <td>capture</td>\n",
       "      <td>film</td>\n",
       "      <td>woman</td>\n",
       "      <td>force</td>\n",
       "      <td>race</td>\n",
       "      <td>classic</td>\n",
       "      <td>plan</td>\n",
       "      <td>stun</td>\n",
       "      <td>hero</td>\n",
       "      <td>set</td>\n",
       "      <td>novel</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>camp</td>\n",
       "      <td>driver</td>\n",
       "      <td>film</td>\n",
       "      <td>escape</td>\n",
       "      <td>french</td>\n",
       "      <td>captain</td>\n",
       "      <td>nicholson</td>\n",
       "      <td>british</td>\n",
       "      <td>american</td>\n",
       "      <td>group</td>\n",
       "      <td>set</td>\n",
       "      <td>new</td>\n",
       "      <td>half</td>\n",
       "      <td>southern</td>\n",
       "      <td>japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>child</td>\n",
       "      <td>family</td>\n",
       "      <td>award</td>\n",
       "      <td>jane</td>\n",
       "      <td>turn</td>\n",
       "      <td>academy</td>\n",
       "      <td>horror</td>\n",
       "      <td>black</td>\n",
       "      <td>home</td>\n",
       "      <td>terrify</td>\n",
       "      <td>film</td>\n",
       "      <td>begin</td>\n",
       "      <td>evil</td>\n",
       "      <td>town</td>\n",
       "      <td>nominee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>eve</td>\n",
       "      <td>life</td>\n",
       "      <td>love</td>\n",
       "      <td>work</td>\n",
       "      <td>world</td>\n",
       "      <td>star</td>\n",
       "      <td>story</td>\n",
       "      <td>rescue</td>\n",
       "      <td>man</td>\n",
       "      <td>way</td>\n",
       "      <td>rovi</td>\n",
       "      <td>meet</td>\n",
       "      <td>home</td>\n",
       "      <td>planet</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>quest</td>\n",
       "      <td>voice</td>\n",
       "      <td>guide</td>\n",
       "      <td>encounter</td>\n",
       "      <td>teenager</td>\n",
       "      <td>band</td>\n",
       "      <td>film</td>\n",
       "      <td>feature</td>\n",
       "      <td>island</td>\n",
       "      <td>haunt</td>\n",
       "      <td>sexual</td>\n",
       "      <td>vision</td>\n",
       "      <td>sense</td>\n",
       "      <td>strange</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>social</td>\n",
       "      <td>explores</td>\n",
       "      <td>century</td>\n",
       "      <td>relationship</td>\n",
       "      <td>struggle</td>\n",
       "      <td>carrie</td>\n",
       "      <td>way</td>\n",
       "      <td>drama</td>\n",
       "      <td>film</td>\n",
       "      <td>special</td>\n",
       "      <td>classic</td>\n",
       "      <td>focus</td>\n",
       "      <td>story</td>\n",
       "      <td>set</td>\n",
       "      <td>rovi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>young</td>\n",
       "      <td>family</td>\n",
       "      <td>time</td>\n",
       "      <td>kill</td>\n",
       "      <td>make</td>\n",
       "      <td>home</td>\n",
       "      <td>mission</td>\n",
       "      <td>order</td>\n",
       "      <td>begin</td>\n",
       "      <td>come</td>\n",
       "      <td>force</td>\n",
       "      <td>men</td>\n",
       "      <td>life</td>\n",
       "      <td>alien</td>\n",
       "      <td>future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>film</td>\n",
       "      <td>best</td>\n",
       "      <td>director</td>\n",
       "      <td>movie</td>\n",
       "      <td>rovi</td>\n",
       "      <td>make</td>\n",
       "      <td>picture</td>\n",
       "      <td>hal</td>\n",
       "      <td>erickson</td>\n",
       "      <td>academy</td>\n",
       "      <td>play</td>\n",
       "      <td>story</td>\n",
       "      <td>version</td>\n",
       "      <td>star</td>\n",
       "      <td>won</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>film</td>\n",
       "      <td>friend</td>\n",
       "      <td>best</td>\n",
       "      <td>movie</td>\n",
       "      <td>focus</td>\n",
       "      <td>son</td>\n",
       "      <td>father</td>\n",
       "      <td>way</td>\n",
       "      <td>ordinary</td>\n",
       "      <td>girl</td>\n",
       "      <td>social</td>\n",
       "      <td>wood</td>\n",
       "      <td>shoot</td>\n",
       "      <td>year</td>\n",
       "      <td>start</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>chance</td>\n",
       "      <td>bob</td>\n",
       "      <td>hotel</td>\n",
       "      <td>adventure</td>\n",
       "      <td>fox</td>\n",
       "      <td>survival</td>\n",
       "      <td>family</td>\n",
       "      <td>war</td>\n",
       "      <td>world</td>\n",
       "      <td>lady</td>\n",
       "      <td>new</td>\n",
       "      <td>trust</td>\n",
       "      <td>fight</td>\n",
       "      <td>famous</td>\n",
       "      <td>change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 10</th>\n",
       "      <td>film</td>\n",
       "      <td>rovi</td>\n",
       "      <td>girl</td>\n",
       "      <td>man</td>\n",
       "      <td>jason</td>\n",
       "      <td>daughter</td>\n",
       "      <td>old</td>\n",
       "      <td>detective</td>\n",
       "      <td>novel</td>\n",
       "      <td>screen</td>\n",
       "      <td>original</td>\n",
       "      <td>hal</td>\n",
       "      <td>mark</td>\n",
       "      <td>police</td>\n",
       "      <td>role</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 11</th>\n",
       "      <td>western</td>\n",
       "      <td>land</td>\n",
       "      <td>killer</td>\n",
       "      <td>owner</td>\n",
       "      <td>arrive</td>\n",
       "      <td>thwart</td>\n",
       "      <td>newly</td>\n",
       "      <td>title</td>\n",
       "      <td>revenge</td>\n",
       "      <td>reason</td>\n",
       "      <td>henry</td>\n",
       "      <td>trilogy</td>\n",
       "      <td>plot</td>\n",
       "      <td>visual</td>\n",
       "      <td>figure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 12</th>\n",
       "      <td>instead</td>\n",
       "      <td>fairy</td>\n",
       "      <td>backdrop</td>\n",
       "      <td>del</td>\n",
       "      <td>army</td>\n",
       "      <td>drop</td>\n",
       "      <td>club</td>\n",
       "      <td>cold</td>\n",
       "      <td>target</td>\n",
       "      <td>dozen</td>\n",
       "      <td>claimed</td>\n",
       "      <td>case</td>\n",
       "      <td>toro</td>\n",
       "      <td>happen</td>\n",
       "      <td>hidden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 13</th>\n",
       "      <td>film</td>\n",
       "      <td>life</td>\n",
       "      <td>story</td>\n",
       "      <td>team</td>\n",
       "      <td>begin</td>\n",
       "      <td>new</td>\n",
       "      <td>young</td>\n",
       "      <td>world</td>\n",
       "      <td>star</td>\n",
       "      <td>family</td>\n",
       "      <td>home</td>\n",
       "      <td>make</td>\n",
       "      <td>come</td>\n",
       "      <td>way</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 14</th>\n",
       "      <td>wood</td>\n",
       "      <td>film</td>\n",
       "      <td>message</td>\n",
       "      <td>come</td>\n",
       "      <td>surf</td>\n",
       "      <td>life</td>\n",
       "      <td>student</td>\n",
       "      <td>horror</td>\n",
       "      <td>witch</td>\n",
       "      <td>know</td>\n",
       "      <td>people</td>\n",
       "      <td>video</td>\n",
       "      <td>young</td>\n",
       "      <td>want</td>\n",
       "      <td>kid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word 0    Word 1    Word 2        Word 3    Word 4    Word 5  \\\n",
       "Topic 0     voice    murder      dead       capture      film     woman   \n",
       "Topic 1      camp    driver      film        escape    french   captain   \n",
       "Topic 2     child    family     award          jane      turn   academy   \n",
       "Topic 3       eve      life      love          work     world      star   \n",
       "Topic 4     quest     voice     guide     encounter  teenager      band   \n",
       "Topic 5    social  explores   century  relationship  struggle    carrie   \n",
       "Topic 6     young    family      time          kill      make      home   \n",
       "Topic 7      film      best  director         movie      rovi      make   \n",
       "Topic 8      film    friend      best         movie     focus       son   \n",
       "Topic 9    chance       bob     hotel     adventure       fox  survival   \n",
       "Topic 10     film      rovi      girl           man     jason  daughter   \n",
       "Topic 11  western      land    killer         owner    arrive    thwart   \n",
       "Topic 12  instead     fairy  backdrop           del      army      drop   \n",
       "Topic 13     film      life     story          team     begin       new   \n",
       "Topic 14     wood      film   message          come      surf      life   \n",
       "\n",
       "             Word 6     Word 7    Word 8   Word 9   Word 10  Word 11  Word 12  \\\n",
       "Topic 0       force       race   classic     plan      stun     hero      set   \n",
       "Topic 1   nicholson    british  american    group       set      new     half   \n",
       "Topic 2      horror      black      home  terrify      film    begin     evil   \n",
       "Topic 3       story     rescue       man      way      rovi     meet     home   \n",
       "Topic 4        film    feature    island    haunt    sexual   vision    sense   \n",
       "Topic 5         way      drama      film  special   classic    focus    story   \n",
       "Topic 6     mission      order     begin     come     force      men     life   \n",
       "Topic 7     picture        hal  erickson  academy      play    story  version   \n",
       "Topic 8      father        way  ordinary     girl    social     wood    shoot   \n",
       "Topic 9      family        war     world     lady       new    trust    fight   \n",
       "Topic 10        old  detective     novel   screen  original      hal     mark   \n",
       "Topic 11      newly      title   revenge   reason     henry  trilogy     plot   \n",
       "Topic 12       club       cold    target    dozen   claimed     case     toro   \n",
       "Topic 13      young      world      star   family      home     make     come   \n",
       "Topic 14    student     horror     witch     know    people    video    young   \n",
       "\n",
       "           Word 13   Word 14  \n",
       "Topic 0      novel    attack  \n",
       "Topic 1   southern  japanese  \n",
       "Topic 2       town   nominee  \n",
       "Topic 3     planet    people  \n",
       "Topic 4    strange    comedy  \n",
       "Topic 5        set      rovi  \n",
       "Topic 6      alien    future  \n",
       "Topic 7       star       won  \n",
       "Topic 8       year     start  \n",
       "Topic 9     famous    change  \n",
       "Topic 10    police      role  \n",
       "Topic 11    visual    figure  \n",
       "Topic 12    happen    hidden  \n",
       "Topic 13       way  american  \n",
       "Topic 14      want       kid  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LatentDirichletAllocation(n_components = 15, # Number of topics\n",
    "                                      max_iter=50,                # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1                # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(vectorized_plots)\n",
    "\n",
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords\n",
    "\n",
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=15)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scraping Plots from IMDb's '1,000 Greatest Films of All Time'\n",
    "\n",
    "The webpage for each movie has a storyline section that we can scrape after figuring out the CSS tag. The IMBD movie webpages look like https://www.imdb.com/title/[code]/ where [code] follows the pattern \"tt1234567\". First we will scrape these codes from the \"Top 1,000\" list, then we will scrape the individual storylines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movie_link_getter(tags):\n",
    "    \"\"\" Returns a list of codes for each movies webpage\"\"\"\n",
    "    link_pattern = re.compile('tt[0-9]{7}')\n",
    "    links = [link_pattern.search(str(tag)) for tag in tags]\n",
    "    return [link.group(0) for link in links]\n",
    "\n",
    "# get the url codes for top 1,000 most best movies on IMBD\n",
    "links = []\n",
    "\n",
    "# webpages are broken up into 100 movie chunks, so we will need to loop through them\n",
    "for start in range(1,11):\n",
    "    url = \"https://www.imdb.com/list/ls006266261/?sort=list_order,asc&st_dt=&mode=detail&page=\" + str(start)\n",
    "    #url = 'https://www.imdb.com/search/title/?groups=top_1000&count=100&start=' + str(start) + '&ref_=adv_nxt'\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    tags = soup.select('.lister-item-header a')\n",
    "    links.extend(movie_link_getter(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "# download proxies so we don't get blocked\n",
    "#proxies = requests.get(\"https://free-proxy-list.net/\")\n",
    "#proxies_soup = BeautifulSoup(proxies.text)\n",
    "#proxies_df = pd.read_html(str(proxies_soup.find_all('table')[0]))[0]\n",
    "#proxies_df.dropna(inplace=True)\n",
    "#proxy_pool = cycle(proxies_df['IP Address'])\n",
    "\n",
    "story_lines = []\n",
    "\n",
    "for link in links[0:10]:\n",
    "    \"https://www.rottentomatoes.com\"\n",
    "    url = 'https://www.imbd.com/title/' + link + \"/plotsummary?ref_=tt_stry_pl\"\n",
    "    #while True:\n",
    "    #    try:\n",
    "    #        proxy = next(proxy_pool) # get a new proxy\n",
    "    #        resp = requests.get(url,proxies={\"http\": proxy, \"https\": proxy})\n",
    "    #    except:\n",
    "    #        continue\n",
    "    #    break\n",
    "    resp = requests.get(url)\n",
    "    soup = BeautifulSoup(resp.text)\n",
    "    synopsis = soup.find_all(id=re.compile(\"^synopsis-.*\"))\n",
    "    \n",
    "    # clean up the text a little\n",
    "    synopsis = re.sub(\"<br/>\", \" \", str(synopsis[0]))\n",
    "    synopsis = re.search(\">(.*)<\", synopsis).group(1)\n",
    "    \n",
    "    story_lines.append(synopsis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
